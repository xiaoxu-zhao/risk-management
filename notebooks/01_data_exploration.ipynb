{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Data Exploration\n",
    "\n",
    "This notebook demonstrates comprehensive data exploration techniques for credit risk datasets.\n",
    "\n",
    "## Objectives:\n",
    "- Load and examine credit risk datasets\n",
    "- Perform data quality assessment\n",
    "- Conduct exploratory data analysis (EDA)\n",
    "- Identify patterns and relationships\n",
    "- Generate insights for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_loader import CreditDataLoader\n",
    "from visualization import RiskVisualizer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = CreditDataLoader(data_path='../data/')\n",
    "\n",
    "# For demonstration, we'll use sample data\n",
    "# In practice, you would load real datasets:\n",
    "# df = loader.load_give_me_credit('../data/cs-training.csv')\n",
    "# df = loader.load_home_credit('../data/application_train.csv')\n",
    "\n",
    "# Generate sample dataset for demonstration\n",
    "df = loader.get_sample_data(n_samples=5000, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Descriptive statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive data quality check\n",
    "quality_report = loader.basic_data_quality_check(df, 'default')\n",
    "\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "print(f\"Dataset shape: {quality_report['shape']}\")\n",
    "print(f\"Total missing values: {quality_report['missing_values']}\")\n",
    "print(f\"Duplicate rows: {quality_report['duplicate_rows']}\")\n",
    "print(f\"Default rate: {quality_report['target_rate']:.2%}\")\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "for value, count in quality_report['target_distribution'].items():\n",
    "    print(f\"  {value}: {count} ({count/sum(quality_report['target_distribution'].values()):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_pct = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"Missing values by column:\")\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count plot\n",
    "df['default'].value_counts().plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'])\n",
    "ax1.set_title('Default Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Default Status')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticklabels(['No Default', 'Default'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['default'].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%', \n",
    "                                  labels=['No Default', 'Default'],\n",
    "                                  colors=['skyblue', 'salmon'])\n",
    "ax2.set_title('Default Distribution (%)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Class imbalance ratio: {df['default'].value_counts()[0] / df['default'].value_counts()[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features distribution\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.drop('default')\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if idx < len(axes):\n",
    "        # Distribution by default status\n",
    "        df[df['default']==0][col].hist(bins=30, alpha=0.7, label='No Default', \n",
    "                                       color='skyblue', ax=axes[idx])\n",
    "        df[df['default']==1][col].hist(bins=30, alpha=0.7, label='Default', \n",
    "                                       color='salmon', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{col} Distribution', fontweight='bold')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Relationships and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "viz = RiskVisualizer()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "correlation_fig = viz.plot_correlation_heatmap(df, figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target variable\n",
    "target_corr = df.corr()['default'].sort_values(key=abs, ascending=False)\n",
    "target_corr = target_corr.drop('default')  # Remove self-correlation\n",
    "\n",
    "print(\"Correlation with default (sorted by absolute value):\")\n",
    "for feature, corr in target_corr.items():\n",
    "    print(f\"{feature:25s}: {corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations with target\n",
    "top_corr_features = target_corr.head(6)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (feature, corr) in enumerate(top_corr_features.items()):\n",
    "    # Box plot by default status\n",
    "    df.boxplot(column=feature, by='default', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature}\\n(Correlation: {corr:.3f})', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Default Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze default rates by risk segments\n",
    "key_features = ['credit_score', 'debt_to_income', 'age', 'income']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df.columns:\n",
    "        # Create risk bins\n",
    "        df[f'{feature}_bin'] = pd.qcut(df[feature], q=10, precision=2, duplicates='drop')\n",
    "        \n",
    "        # Calculate default rates by bin\n",
    "        bin_stats = df.groupby(f'{feature}_bin')['default'].agg(['count', 'mean']).reset_index()\n",
    "        bin_stats.columns = [f'{feature}_bin', 'count', 'default_rate']\n",
    "        \n",
    "        # Plot default rates\n",
    "        x_pos = range(len(bin_stats))\n",
    "        bars = axes[idx].bar(x_pos, bin_stats['default_rate'], \n",
    "                            color='lightcoral', alpha=0.7)\n",
    "        axes[idx].set_title(f'Default Rate by {feature}', fontweight='bold')\n",
    "        axes[idx].set_ylabel('Default Rate')\n",
    "        axes[idx].set_xlabel(f'{feature} Bins')\n",
    "        axes[idx].set_xticks(x_pos)\n",
    "        axes[idx].set_xticklabels([f'Q{i+1}' for i in x_pos], rotation=45)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{height:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk score based on key features\n",
    "def calculate_risk_score(row):\n",
    "    \"\"\"Simple risk scoring function for demonstration.\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Credit score component (lower is worse)\n",
    "    if row['credit_score'] < 600:\n",
    "        score += 40\n",
    "    elif row['credit_score'] < 700:\n",
    "        score += 20\n",
    "    \n",
    "    # Debt-to-income component\n",
    "    if row['debt_to_income'] > 0.4:\n",
    "        score += 30\n",
    "    elif row['debt_to_income'] > 0.3:\n",
    "        score += 15\n",
    "    \n",
    "    # Age component (very young or old)\n",
    "    if row['age'] < 25 or row['age'] > 65:\n",
    "        score += 10\n",
    "    \n",
    "    # Late payment component\n",
    "    if row['num_late_payments'] > 2:\n",
    "        score += 20\n",
    "    elif row['num_late_payments'] > 0:\n",
    "        score += 10\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Calculate risk scores\n",
    "df['risk_score'] = df.apply(calculate_risk_score, axis=1)\n",
    "\n",
    "# Analyze risk score performance\n",
    "risk_bins = pd.cut(df['risk_score'], bins=5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "df['risk_category'] = risk_bins\n",
    "\n",
    "risk_analysis = df.groupby('risk_category')['default'].agg(['count', 'sum', 'mean']).round(3)\n",
    "risk_analysis.columns = ['Total', 'Defaults', 'Default_Rate']\n",
    "risk_analysis['Default_Rate_Pct'] = (risk_analysis['Default_Rate'] * 100).round(1)\n",
    "\n",
    "print(\"Risk Score Performance:\")\n",
    "display(risk_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk score performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Default rates by risk category\n",
    "risk_analysis['Default_Rate'].plot(kind='bar', ax=ax1, color='lightcoral', alpha=0.7)\n",
    "ax1.set_title('Default Rate by Risk Category', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Default Rate')\n",
    "ax1.set_xlabel('Risk Category')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Volume by risk category\n",
    "risk_analysis['Total'].plot(kind='bar', ax=ax2, color='lightblue', alpha=0.7)\n",
    "ax2.set_title('Volume by Risk Category', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('Risk Category')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY INSIGHTS FROM DATA EXPLORATION ===\")\n",
    "print()\n",
    "\n",
    "# Dataset characteristics\n",
    "print(f\"1. Dataset Overview:\")\n",
    "print(f\"   - Total observations: {len(df):,}\")\n",
    "print(f\"   - Features: {len(df.columns)-1}\")\n",
    "print(f\"   - Default rate: {df['default'].mean():.2%}\")\n",
    "print(f\"   - Class imbalance: {df['default'].value_counts()[0]/df['default'].value_counts()[1]:.1f}:1\")\n",
    "print()\n",
    "\n",
    "# Feature insights\n",
    "print(f\"2. Most Predictive Features (by correlation):\")\n",
    "for i, (feature, corr) in enumerate(target_corr.head(5).items()):\n",
    "    print(f\"   {i+1}. {feature}: {corr:.3f}\")\n",
    "print()\n",
    "\n",
    "# Risk segmentation insights\n",
    "print(f\"3. Risk Segmentation Performance:\")\n",
    "low_risk_rate = risk_analysis.loc['Low', 'Default_Rate']\n",
    "high_risk_rate = risk_analysis.loc['High', 'Default_Rate']\n",
    "print(f\"   - Low risk default rate: {low_risk_rate:.2%}\")\n",
    "print(f\"   - High risk default rate: {high_risk_rate:.2%}\")\n",
    "print(f\"   - Risk discrimination: {high_risk_rate/low_risk_rate:.1f}x higher\")\n",
    "print()\n",
    "\n",
    "print(f\"4. Recommendations for Model Development:\")\n",
    "print(f\"   - Address class imbalance using techniques like SMOTE or class weighting\")\n",
    "print(f\"   - Focus feature engineering on top predictive variables\")\n",
    "print(f\"   - Consider ensemble methods for better performance\")\n",
    "print(f\"   - Implement proper cross-validation for model selection\")\n",
    "print(f\"   - Monitor model calibration for probability accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this exploration, the next notebooks will cover:\n",
    "\n",
    "1. **Feature Engineering** (`02_feature_engineering.ipynb`):\n",
    "   - Advanced feature transformations\n",
    "   - Interaction terms creation\n",
    "   - Missing value imputation strategies\n",
    "\n",
    "2. **Model Development** (`03_model_development.ipynb`):\n",
    "   - Multiple algorithm comparison\n",
    "   - Hyperparameter optimization\n",
    "   - Model validation and selection\n",
    "\n",
    "3. **Risk Analysis** (`04_risk_analysis.ipynb`):\n",
    "   - Portfolio risk calculations\n",
    "   - Regulatory capital estimation\n",
    "   - Stress testing scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}